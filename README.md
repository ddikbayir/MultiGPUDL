# MultiGPUDL

Training phase in deep learning is a computationally expensive operation and may take up to weeks even on decent GPU devices. This fact makes the optimization of the training algorithm in the network extremely important. Hardware accelerators like FPGAs and GPUs are mainly used for speeding up network training. However, these devices usually have low on-chip memory and therefore frequently fetch data from CPU which results in greater communication amount and decrease in the performance. Reducing this communication amount between hardware accelerators and CPU requires both algorithm and architecture level optimization strategies. In this project, I will try to analyze and understand different parallelization and optimization strategies and apply them to speed up training.
